<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Welcome to the WMT 2023 Metrics Shared Task! &middot; WMT23 Metrics Task
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          WMT23 Metrics Task
        </a>
      </h1>
      <p class="lead">This shared task will examine automatic evaluation metrics for machine translation.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item active" href="/">Metrics Task</a>
      

      
      
        
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/subtasks/challenge/">Challenge Sets Subtask</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/subtasks/previous/">Previous Editions</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/subtasks/qe/">QE as a Metric</a>
          
        
      
        
      
        
      
      
    </nav>

    <p>&copy; 2023. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <h1 id="welcome-to-the-wmt-2023-metrics-shared-task">Welcome to the WMT 2023 Metrics Shared Task!</h1>

<p class="message">
  This shared task will examine automatic evaluation metrics for machine translation. We will provide you with MT system outputs along with source text and the human reference translations. We are looking for automatic metric scores for translations at the system-level, and segment-level. We will calculate the system-level, and segment-level correlations of your scores with human judgements.<br /><br />

  We invite submissions of <strong>reference-free metrics</strong> in addition to <strong>reference-based metrics</strong>.<br /><br />   
  
  Have questions or suggestions? Feel free to <a href="mailto:wmt-metrics@googlegroups.com">Contact Us</a>!
</p>

<h2 id="new-metric-inputs-and-codalab-release">NEW: Metric inputs and Codalab release:</h2>

<ol>
  <li>Register your metric <a href="https://forms.gle/UTBen7EBRJaMFttK6">here</a>, if you haven’t already</li>
  <li>Create an account on Codalab.
    <ul>
      <li>You’re allowed one primary submission for a reference-based metric, and one primary submission for a reference-free metric. If you are submitting two metrics that have <strong>widely different approaches</strong>, for example, one LLM-based metric and one lexical metric, then create 2 accounts on Codalab.</li>
    </ul>
  </li>
  <li>Download the data (<a href="https://drive.google.com/file/d/1oRrZI9nrM3739tdq0PDYaYOr94X2PIgQ/view?usp=sharing">link</a>; link also available on Codalab)</li>
  <li>Prepare your scores:
    <ul>
      <li>Please follow the guidelines on submission format as described bellow. The metric inputs download includes sample metrics as well as helper scripts to prepare your scores.</li>
    </ul>
  </li>
  <li>Submit your scores via <a href="(https://codalab.lisn.upsaclay.fr/competitions/15074)">Codalab</a>:
    <ul>
      <li>When you submit your metric, Codalab might require some time to process your  submission. We’ve noticed processing times between a few minutes and two hours when testing. Codalab does keep track of the submission time, so don’t panic if your last minute submission wasn’t processed before the deadline! Please contact us if it has been longer than 3 hours.</li>
      <li>After uploading your submission, check its status (under Submit / View Results). It will return an error if there’s an issue with submission, such as formatting issues</li>
    </ul>
  </li>
</ol>

<p>Deadline for submissions is <strong>17th August, 2023 ❗</strong>. Please check the dates bellow.</p>

<h2 id="important-links">Important links</h2>

<p><a href="https://github.com/google-research/mt-metrics-eval">mt-metrics-eval</a>: the tool for calculating correlation numbers aka the sacreBLEU for metric developers. You can also dump the most recent test sets.</p>

<p><a href="https://codalab.lisn.upsaclay.fr/competitions/15074"><strong>NEW</strong>: Codalab submission platform</a></p>

<h2 id="important-dates">Important Dates</h2>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">Date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Breaking round for <a href="./subtasks/challenge/">challenge sets</a></td>
      <td style="text-align: center"><strong>25th July, 2023</strong></td>
    </tr>
    <tr>
      <td><strong>System outputs ready to download</strong></td>
      <td style="text-align: center"><strong>10th August, 2023</strong></td>
    </tr>
    <tr>
      <td><strong>Submission deadline for metrics task</strong></td>
      <td style="text-align: center"><strong>17th August, 2023 ❗</strong></td>
    </tr>
    <tr>
      <td>Paper submission deadline to WMT</td>
      <td style="text-align: center">5th September, 2023</td>
    </tr>
    <tr>
      <td>WMT Notification of acceptance</td>
      <td style="text-align: center">6th October, 2023</td>
    </tr>
    <tr>
      <td>WMT Camera-ready deadline</td>
      <td style="text-align: center">18th October, 2023</td>
    </tr>
    <tr>
      <td>Conference</td>
      <td style="text-align: center">6th - 7th December, 2023</td>
    </tr>
  </tbody>
</table>

<h2 id="goals">Goals</h2>

<p>The goals of the shared metrics task are:</p>

<ul>
  <li>To achieve the strongest correlation with human judgement of translation quality over a diverse set of MT systems;</li>
  <li>To illustrate the suitability of an automatic evaluation metric as a surrogate for human evaluation;</li>
  <li>To test robustness of metrics when evaluating domains other than news data;</li>
  <li>To create high quality datasets for developing and evaluating metrics</li>
</ul>

<h2 id="task-description">Task Description</h2>

<p>We will provide you with the source sentences, output of machine translation systems and reference translations.</p>

<ol>
  <li>Official results: Correlation with MQM scores at the sentence and system level for the following language pairs:
    <ul>
      <li>Hebrew-English <strong>(NEW!)</strong></li>
      <li>Chinese-English</li>
      <li>English-German <strong>This will be a paragraph-level task!</strong></li>
    </ul>
  </li>
  <li>Secondary Evaluation: Correlation with official <a href="http://www2.statmt.org/wmt23/translation-task.html">WMT Human Evaluation</a> at the sentence and system level.</li>
</ol>

<h3 id="subtasks">Subtasks:</h3>

<ol>
  <li><a href="./subtasks/qe/">QE as a Metric</a>: In this subtask participants have to score machine translation systems without access to reference translations</li>
  <li><a href="./subtasks/challenge/">Challenge Sets</a>: While other participants are worried with building stronger and better metrics, participants of this subtask have to build challengesets that identify where metrics fail!</li>
</ol>

<h2 id="how-to-participate">How to participate?</h2>

<p>Please fill the following <a href="https://forms.gle/UTBen7EBRJaMFttK6">registration form</a> so we can keep track of participants. Since we will be using <a href="https://codalab.lisn.upsaclay.fr/">Codalab</a> to handle all submissions you will also have to create an account on codalab and enroll in the competition.</p>

<h3 id="paper-describing-your-metric">Paper Describing Your Metric</h3>

<p>You are invited to submit a short paper (4 to 6 pages) to WMT describing your automatic evaluation metric. Shared task submission description papers are non-archival, and you are not required to submit a paper if you do not want to. If you don’t, we ask that you give an appropriate reference describing your metric that we can cite in the overview paper.</p>

<h3 id="submission-format">Submission Format</h3>

<p>The output of your software should produce scores for the translations either at the system-level or the segment-level (or preferably both).</p>

<p><strong>Output file format for system-level rankings</strong></p>

<p>The output files for system-level scores should be called YOURMETRIC.sys.score and formatted as tab-separated values (TSV) in the following way:</p>

<blockquote>
  <p>METRIC-NAME\tLANG-PAIR\tTESTSET\tDOMAIN\tREFERENCE\tSYSTEM-ID\tSYSTEM-SCORE</p>
</blockquote>

<p><strong>Output file format for segment-level scores</strong></p>

<p>The output files for segment-level scores should be called YOURMETRIC.seg.score and formatted as tab-separated values (TSV) in the following way:</p>

<blockquote>
  <p>METRIC-NAME\tLANG-PAIR\tTESTSET\tDOMAIN\tDOCUMENT\tREFERENCE\tSYSTEM-ID\tSEGMENT-NUMBER\tSEGMENT-SCORE</p>
</blockquote>

<p>Each field should be delimited by a single tab character.</p>

<p>Where:</p>
<ul>
  <li><strong>METRIC-NAME</strong> is the name of your automatic evaluation metric. Please use short descriptive names that don’t contain spaces.</li>
  <li><strong>LANG-PAIR</strong> is the language pair using two letter abbreviations for the languages (e.g., he-en, zh-en, and en-de for the official language pairs).</li>
  <li><strong>TESTSET</strong> is the ID of the test set (e.g., generaltest2023, or challenge*).</li>
  <li><strong>DOMAIN</strong> is the domain of a given segment. For he-en, zh-en, and en-de, domain should be one of conversation, ecommerce, news, social or all. For other language pairs, use all. For system-level scores, this field designates the domain that is being scored (all means the whole test set). For segment-level scores, DOMAIN is used for verification purposes only.</li>
  <li><strong>DOCUMENT</strong> is the ID of the document. This field is used for verification only.</li>
  <li><strong>REFERENCE</strong> is the ID of the reference (refA, refB, etc for reference-based metrics, and src for reference-free metrics).</li>
  <li><strong>SYSTEM-ID</strong> is the ID of the system being scored (given by the part of the filename for the plain text file, for example “uedin-syntax”).</li>
  <li><strong>SEGMENT-NUMBER</strong> is the line number starting from 1 of the plain text input files.</li>
  <li><strong>SYSTEM-SCORE</strong> is the score your metric predicts for the particular system.</li>
  <li><strong>SEGMENT-SCORE</strong> is the score your metric predicts for the particular segment.</li>
</ul>

<h4 id="codalab">Codalab:</h4>

<p>This year we will be using <a href="https://codalab.lisn.upsaclay.fr/">Codalab</a> to handle all submissions.</p>

<p>Create a zip archive containing YOURMETRIC.sys.score and/or YOURMETRIC.seg.score, and upload it using the Submit / View Results tab under Participate. Fill in the meta-information for your team and metric, as prompted.</p>

<p>Each submission should contain scores for only one metric. The official metric name is the one that appears in the METRIC-NAME field in the score files.</p>

<p>Reference-free (aka QE) metrics must have <code class="language-plaintext highlighter-rouge">src</code> in the REFERENCE field; if a metric is reference free, it must be reference-free for both system- and segment-level scores.</p>

<p>If your submission contains only segment-level scores, we will fill in system-level scores by averaging.</p>

<p>You can make multiple submissions. Each new submission must have a different metric name, and one submission must be designated as primary. Only this submission will participate in the official evaluation (final metric ranking). To designate a submission as primary, include PRIMARY in the Description field. You can update this field to change your primary submission at any time before the evaluation ends.</p>

<p>Primary submissions must include, at minimum, segment-level scores for all official language pairs (Hebrew-English, Chinese-English, English-German).</p>

<p><strong>Verify submission:</strong></p>

<p>After uploading your submission, check that its status (under Submit / View Results) shows Finished and a numerical score appears in the Score column. This can take some time. <strong>Use the Results tab to check your correlation scores on the leaderboard. These are correlations with an automatic metric, and will not reflect your final correlations to human MQM scores, nor your true ranking compared to other submissions.However, if they are very low or negative, it could indicate a problem with your scores.</strong></p>

<p>If there is a problem uploading your submission, its status will be Failed, and an Error display will show the reasons for the failure. You can get other information from the links under the Error panel. You can also test your submission offline by running the <a href="https://github.com/google-research/mt-metrics-eval/blob/main/mt_metrics_eval/codalab/eval.py">scoring script</a> yourself.</p>

<p>There is currently no way to remove failed submissions or replace existing valid submissions from codalab. To indicate that you do not wish us to use a submission, include DISCARD in its Description field.</p>

<h2 id="training-data">Training Data</h2>

<p>Since data from previous WMT editions might be difficult to navigate we uploaded previous years data to Hugging Face Datasets. You can find DA, MQM and SQM annotations from previous years in the following links: <a href="https://huggingface.co/datasets/RicardoRei/wmt-da-human-evaluation">wmt-da-human-evaluation</a>, <a href="https://huggingface.co/datasets/RicardoRei/wmt-mqm-human-evaluation">wmt-mqm-human-evaluation</a>, <a href="https://huggingface.co/datasets/RicardoRei/wmt-sqm-human-evaluation">wmt-sqm-human-evaluation</a>.</p>

<p>If you wish to find the original data please check the <a href="https://wmt-metrics-task.github.io/subtasks/previous/">previous editions tab</a> and in the results section you can find the original DAs. For MQM you can find the data <a href="https://github.com/google/wmt-mqm-human-evaluation">here</a></p>

<h2 id="organization">Organization:</h2>

<ul>
  <li>Alon Lavie, Unbabel</li>
  <li>Brian Thompson, Amazon Research</li>
  <li>Chi-kiu (Jackie) Lo, NRC Canada</li>
  <li>Chryssa Zerva, Instituto de Telecomunicações and Instituto Superior Técnico</li>
  <li>Craig Stewart, Unbabel</li>
  <li>Dan Deutsch, Google Research</li>
  <li>Eleftherios Avramidis, German Research Center for Artificial Intelligence (DFKI)</li>
  <li>Frédéric Blain, Tilburg University</li>
  <li>George Foster, Google Research</li>
  <li>Markus Freitag, Google Research</li>
  <li>Nitika Mathur, Oracle</li>
  <li>Ricardo Rei, Unbabel, INESC-ID and Instituto Superior Técnico</li>
  <li>Sheila Castilho, ADAPT Centre - Dublin City University</li>
  <li>Tom Kocmi, Microsoft Research</li>
</ul>

<h2 id="sponsors">Sponsors</h2>

<style>
	.column {
	  float: left;
	  padding: 20px;
	}
	
</style>

<div style="position: relative; width: 700px; height: 100px; min-height: 200px">    
    <div style="position: relative; bottom: 0px;">
	   <div class="column">
	     <img src="/public/css/google.png" height="70px" width="auto" />
	   </div>
	   <div class="column">
	     <img src="/public/css/unbabel.png" height="70px" width="auto" />
	   </div>
	   <div class="column">
	     <img src="/public/css/microsoft.jpeg" height="70px" width="auto" />
	   </div>
	</div>

<div style="position: relative; width: 700px; height: 100px; min-height: 200px">    
    <div style="position: relative; bottom: 0px;">
	   <div class="column">
	     <img src="/public/css/NRC-logo.png" height="70px" width="auto" />
	   </div>
	   <div class="column">
	     <img src="/public/css/IST.png" height="70px" width="auto" />
	   </div>
	   <div class="column">
	     <img src="/public/css/DFKI.jpeg" height="50px" width="auto" />
	   </div>
	   <div class="column">
	     <img src="/public/css/ADAPT.png" height="50px" width="auto" />
	   </div>
	</div>
</div></div>

    </div>

  </body>
</html>
